{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi9bo5pLwvhb",
        "outputId": "ddc916b9-ab5a-4865-ec09-302f9e9fafa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-13 05:52:40.501 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-13 05:52:40.503 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner Streamlit output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Interactive Movie Recommendation Dashboard\")\n",
        "\n",
        "# --- DATA PROCESSING AND MODELING FUNCTIONS (Cached for performance) ---\n",
        "\n",
        "@st.cache_data\n",
        "def load_and_preprocess_data(file_path=\"movies.csv\"):\n",
        "    \"\"\"Loads the data and performs all cleaning/feature engineering steps from the notebook.\"\"\"\n",
        "    try:\n",
        "        # Load data\n",
        "        data = pd.read_csv(file_path, low_memory=False)\n",
        "        st.success(\"âœ… Dataset loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        st.error(f\"Error: The file '{file_path}' was not found. Please ensure 'movies.csv' is in the same directory.\")\n",
        "        return None\n",
        "\n",
        "    # Create a copy and clean columns as per P2code.ipynb\n",
        "    df = data.copy()\n",
        "\n",
        "    # Drop unnecessary columns (Cell 4 logic)\n",
        "    cols_to_drop = [\n",
        "        'homepage', 'production_companies', 'production_countries',\n",
        "        'spoken_languages', 'status', 'original_title', 'index', 'crew'\n",
        "    ]\n",
        "    df = df.drop(cols_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "    # Impute missing values (Cell 6 logic)\n",
        "    for col in ['genres', 'keywords', 'overview', 'tagline', 'cast', 'director']:\n",
        "        df[col] = df[col].fillna('')\n",
        "\n",
        "    # Drop rows with single missing values in crucial columns (Cell 7 logic - single dropna)\n",
        "    df.dropna(subset=['release_date', 'runtime'], inplace=True)\n",
        "\n",
        "    # Convert features to suitable string format (Cell 7 logic)\n",
        "    features = ['keywords', 'cast', 'genres', 'director']\n",
        "    for feature in features:\n",
        "        # Check if the column needs literal_eval (as might be needed for genres/keywords/cast)\n",
        "        # Assuming the data is simplified string format post-cleaning, but handling lists if present\n",
        "        try:\n",
        "             # This part simplifies the complex parsing often found in notebooks\n",
        "            df[feature] = df[feature].apply(lambda x: [i['name'] for i in literal_eval(x)] if isinstance(x, str) and '[' in x else x)\n",
        "        except (ValueError, TypeError):\n",
        "             # If not list-like string, continue as simple string\n",
        "            pass\n",
        "\n",
        "        df[feature] = df[feature].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
        "        df[feature] = df[feature].apply(lambda x: x.lower().replace(\" \", \"\"))\n",
        "\n",
        "    # Create the 'combined_features' string for TF-IDF\n",
        "    def create_soup(x):\n",
        "        # Concatenate relevant features, replacing NaN with empty string\n",
        "        return x['keywords'] + ' ' + x['cast'] + ' ' + x['genres'] + ' ' + x['director']\n",
        "\n",
        "    df['combined_features'] = df.apply(create_soup, axis=1)\n",
        "\n",
        "    # Filter data for modeling (requires non-zero revenue/budget and minimum vote count)\n",
        "    df_modeling = df[(df['budget'] > 0) & (df['revenue'] > 0) & (df['vote_count'] >= 10)].copy()\n",
        "\n",
        "    return df, df_modeling\n",
        "\n",
        "# --- CORE RECOMMENDATION SYSTEM CLASSES ---\n",
        "\n",
        "class ContentRecommender:\n",
        "    \"\"\"Manages the Content-Based Filtering using TF-IDF.\"\"\"\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.tfidf = TfidfVectorizer(stop_words='english')\n",
        "        self.tfidf_matrix = self.tfidf.fit_transform(self.df['combined_features'])\n",
        "        self.cosine_sim = linear_kernel(self.tfidf_matrix, self.tfidf_matrix)\n",
        "        self.indices = pd.Series(self.df.index, index=self.df['title']).drop_duplicates()\n",
        "\n",
        "    def get_recommendations(self, title, n_recs=50):\n",
        "        \"\"\"Generates content-based recommendations.\"\"\"\n",
        "        if title not in self.indices:\n",
        "            return pd.DataFrame({'title': [f\"Movie '{title}' not found in database.\"], 'score': [0.0], 'description': [\"\"]})\n",
        "\n",
        "        idx = self.indices[title]\n",
        "        sim_scores = list(enumerate(self.cosine_sim[idx]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        # Get the scores of the most similar movies (excluding itself)\n",
        "        sim_scores = sim_scores[1:n_recs+1]\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "        recommendations = self.df[['title', 'overview']].iloc[movie_indices].copy()\n",
        "        recommendations['content_score'] = [i[1] for i in sim_scores]\n",
        "        recommendations.rename(columns={'overview': 'description'}, inplace=True)\n",
        "        return recommendations.reset_index(drop=True)\n",
        "\n",
        "class MockLLMQueryInterpreter:\n",
        "    \"\"\"Mocks the Generative AI component (LLM) to extract filtering criteria.\"\"\"\n",
        "    def __init__(self, df):\n",
        "        # Create a set of all unique genres for keyword matching\n",
        "        genres_list = df['genres'].apply(lambda x: re.split('(?=[A-Z])', x) if x else ['']).apply(lambda x: [i.lower() for i in x if i])\n",
        "        self.all_genres = set(g for sub in genres_list for g in sub)\n",
        "\n",
        "    def interpret_query(self, query):\n",
        "        interpretation = {\n",
        "            'genres': [],\n",
        "            'rating_preference': None, # 'high' or 'low'\n",
        "            'popularity_preference': None, # 'popular' or 'niche'\n",
        "        }\n",
        "\n",
        "        query_lower = query.lower()\n",
        "        # 1. Genre extraction\n",
        "        for genre in self.all_genres:\n",
        "            if genre and genre in query_lower:\n",
        "                interpretation['genres'].append(genre)\n",
        "\n",
        "        # 2. Rating preference\n",
        "        if any(word in query_lower for word in ['high rating', 'best rated', 'highly rated', 'critically acclaimed']):\n",
        "            interpretation['rating_preference'] = 'high'\n",
        "        elif any(word in query_lower for word in ['low rating', 'underrated', 'bad rating']):\n",
        "            interpretation['rating_preference'] = 'low'\n",
        "\n",
        "        # 3. Popularity preference\n",
        "        if any(word in query_lower for word in ['popular', 'mainstream', 'widely seen', 'most watched']):\n",
        "            interpretation['popularity_preference'] = 'popular'\n",
        "        elif any(word in query_lower for word in ['niche', 'underground', 'less known', 'obscure']):\n",
        "            interpretation['popularity_preference'] = 'niche'\n",
        "\n",
        "        return interpretation\n",
        "\n",
        "class HybridRecommender:\n",
        "    \"\"\"Combines Content-Based Filtering with criteria filtering based on LLM interpretation.\"\"\"\n",
        "    def __init__(self, df, content_recommender, mock_llm, pred_model=None):\n",
        "        self.df = df\n",
        "        self.content_recommender = content_recommender\n",
        "        self.mock_llm = mock_llm\n",
        "        self.pred_model = pred_model\n",
        "\n",
        "    def get_hybrid_recommendations(self, movie_title, query, n_recs=10):\n",
        "        # 1. Get initial content-based recommendations\n",
        "        content_recs_all = self.content_recommender.get_recommendations(movie_title, n_recs=50)\n",
        "\n",
        "        if 'not found' in content_recs_all['title'].iloc[0]:\n",
        "            return content_recs_all\n",
        "\n",
        "        # 2. Interpret the natural language query\n",
        "        interpretation = self.mock_llm.interpret_query(query)\n",
        "\n",
        "        # 3. Apply filters to initial candidates\n",
        "        # Map back to the full dataset indices for filtering columns\n",
        "        rec_titles = content_recs_all['title'].tolist()\n",
        "        filtered_df = self.df[self.df['title'].isin(rec_titles)].copy()\n",
        "\n",
        "        # Filtering based on LLM interpretation\n",
        "        if interpretation.get('genres'):\n",
        "            genre_filter = filtered_df['genres'].apply(lambda x: any(g in x for g in interpretation['genres']))\n",
        "            filtered_df = filtered_df[genre_filter]\n",
        "\n",
        "        if interpretation.get('rating_preference') == 'high':\n",
        "            min_rating = self.df['vote_average'].quantile(0.75)\n",
        "            filtered_df = filtered_df[filtered_df['vote_average'] >= min_rating]\n",
        "        elif interpretation.get('rating_preference') == 'low':\n",
        "            max_rating = self.df['vote_average'].quantile(0.25)\n",
        "            filtered_df = filtered_df[filtered_df['vote_average'] <= max_rating]\n",
        "\n",
        "        if interpretation.get('popularity_preference') == 'popular':\n",
        "            min_pop = self.df['popularity'].median()\n",
        "            filtered_df = filtered_df[filtered_df['popularity'] >= min_pop]\n",
        "        elif interpretation.get('popularity_preference') == 'niche':\n",
        "            max_pop = self.df['popularity'].median()\n",
        "            filtered_df = filtered_df[filtered_df['popularity'] < max_pop]\n",
        "\n",
        "        # 4. Final Scoring and Selection\n",
        "        if filtered_df.empty:\n",
        "            st.warning(\"âš ï¸ No movies matched both the content and filtering criteria. Showing only top content recommendations.\")\n",
        "            final_recs = content_recs_all.head(n_recs).copy()\n",
        "            final_recs['final_score'] = final_recs['content_score']\n",
        "        else:\n",
        "            final_recs = filtered_df.merge(content_recs_all[['title', 'content_score']], on='title', how='left')\n",
        "\n",
        "            # Simplified Hybrid Score (Content Score + Weighted Popularity)\n",
        "            max_pop_all = self.df['popularity'].max()\n",
        "            final_recs['normalized_popularity'] = final_recs['popularity'] / max_pop_all\n",
        "            # Weighted average: 60% Content, 40% Popularity proxy for model score\n",
        "            final_recs['final_score'] = (0.6 * final_recs['content_score']) + (0.4 * final_recs['normalized_popularity'])\n",
        "\n",
        "            final_recs = final_recs.sort_values('final_score', ascending=False).head(n_recs)\n",
        "            final_recs = final_recs[['title', 'final_score', 'overview']].rename(columns={'overview': 'description'})\n",
        "\n",
        "        return final_recs\n",
        "\n",
        "# --- MODEL TRAINING (Cached Resource) ---\n",
        "@st.cache_resource\n",
        "def train_classification_model(df_modeling):\n",
        "    \"\"\"Trains the classification models for the Evaluation page.\"\"\"\n",
        "    st.info(\"âš™ï¸ Training classification models for evaluation metrics...\")\n",
        "\n",
        "    # Feature Engineering for Classification (Cell 16 logic)\n",
        "    R = df_modeling['vote_average'].mean() # Overall mean rating\n",
        "    V_m = df_modeling['vote_count'].median() # Overall median vote count\n",
        "\n",
        "    df_modeling['recommendable'] = np.where(\n",
        "        (df_modeling['vote_average'] >= R) & (df_modeling['vote_count'] >= V_m), 1, 0\n",
        "    )\n",
        "\n",
        "    # Feature and Target Selection (Cell 17 logic)\n",
        "    numerical_features = ['budget', 'popularity', 'runtime', 'vote_average', 'vote_count']\n",
        "    categorical_features = ['original_language']\n",
        "    all_features = numerical_features + categorical_features\n",
        "    X = df_modeling[all_features].fillna(0)\n",
        "    y = df_modeling['recommendable']\n",
        "\n",
        "    # Splitting Data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Preprocessing Pipeline (Cell 18 logic)\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # Model Pipelines (Cell 19 logic) - XGBoost is the primary model\n",
        "    xgb_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))])\n",
        "    lr_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('classifier', LogisticRegression(random_state=42))])\n",
        "    rf_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "    # Fit models\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    return xgb_model, lr_model, rf_model, X_test, y_test, df_modeling\n",
        "\n",
        "# --- DASHBOARD PAGES ---\n",
        "\n",
        "def page_recommendation_engine(hybrid_recommender, df):\n",
        "    st.title(\"ğŸ¬ Interactive Movie Recommendation Engine\")\n",
        "    st.markdown(\"\"\"\n",
        "    This hybrid recommender system combines **Content-Based Filtering** (using movie metadata like cast, director, genres, and keywords) with **Natural Language Query Filtering** (powered by a mock Generative AI interpreter) to provide context-aware movie suggestions.\n",
        "    \"\"\")\n",
        "    st.divider()\n",
        "\n",
        "    col1, col2 = st.columns([1, 1])\n",
        "\n",
        "    with col1:\n",
        "        movie_title = st.selectbox(\n",
        "            \"1. Select a Movie to Base Recommendations On:\",\n",
        "            options=[''] + sorted(df['title'].unique().tolist()),\n",
        "            index=0\n",
        "        )\n",
        "\n",
        "    with col2:\n",
        "        query = st.text_input(\n",
        "            \"2. Enter a Natural Language Query for Filtering:\",\n",
        "            placeholder=\"e.g., highly rated action movies with high budget\"\n",
        "        )\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    if st.button(\"âœ¨ Get Hybrid Recommendations\", type=\"primary\"):\n",
        "        if not movie_title:\n",
        "            st.warning(\"Please select a base movie.\")\n",
        "            return\n",
        "\n",
        "        with st.spinner(f\"Generating hybrid recommendations based on '{movie_title}' and query: '{query}'...\"):\n",
        "            recommendations = hybrid_recommender.get_hybrid_recommendations(\n",
        "                movie_title, query, n_recs=10\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_1",
        "outputId": "d14868ac-b53d-4c16-c33c-9064ede567de"
      },
      "source": [
        "!pip install streamlit\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.11.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.51.0\n"
          ]
        }
      ]
    }
  ]
}